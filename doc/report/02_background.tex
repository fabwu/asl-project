\section{Background on the Algorithm/Application}\label{sec:background}
To introduce the algorithm, we first touch a glimpse of the underlying theory, then show how one can apply
it to compress images and then finally show practical considerations for the implementation. \\
Many parts of this chapter uses notations from Fisher \cite{fisher2012}.
\mypar{Iterated function systems (IFS)}
Fractal image compression builds on the theory of iterated function systems (IFS). For completeness,
we provide a short introduction to this richt topic.
\begin{definition}[Contractivity of functions]
    A function $f$ on a metric space $X$ with metric $d$ is contractive, when there exists some $0 \leq k < 1$
    such that for all $x,y \in X: d(f(x), f(y)) \leq k \cdot d(x,y)$.
\end{definition}
As an informal example, when $f$ is a contractive function on $\R$, then mapping two numbers brings them \textit{closer} together,
i.e. their distance gets smaller.
\begin{definition}[Iterated function system]
    An iterated function system (IFS) is a set of $n$ functions $$\{f_i:X \to X | i=1,...,n\}$$
    where each $f_i$ is contractive and $X$ is a metric space with metric function $d$.
\end{definition}
\begin{definition}[Hutchinson operator]
    The Hutchinson operater for an IFS is the function
    \begin{align*}
        F \colon 2^X &\to 2^X\\
        F(.) &= \bigcup_{i=1}^n f_i(.)
    \end{align*}
\end{definition}
One can iteratevely apply the Hutchinson operator multiple times on its input. More precisely,
$F^{\circ n}(.)$ means that we are applying $F$ $n$ times on its inputs iteratively.
\begin{definition}[Attractor of an IFS]
    Let $A \in 2^X$. When $F(A)=A$, then $A$ is called is called attractor.
\end{definition}
The following theorem about attractors has been proved by Hutchinson in \cite{hutchinson1981fractals} and is
the essential theoretical part how fractal image compression works.
\begin{theorem}[Contractive mapping fixed-point theorem]\label{theorem-fixpoint}
    For an IFS on a set $X$ it holds that:
    \begin{enumerate}
        \item There always exists an attractor $A$ which is unique
        \item For any nonempty set $S_0 \in X$ it holds that $A = \lim_{n \to \infty} F^{\circ n}(S_0)$
    \end{enumerate}
\end{theorem}
Note that in the theorem above, we require that $X$ is a compact set, whose definition is not relevant for fractal image compression.
\\
\mypar{From IFS to image compression}
The key idea in fractal image compression is that one computes a set of contractive functions $w_i \in \boldsymbol{W}$ (called transformations) on the image.
From theorem \ref{theorem-fixpoint}, we then know that some unique attractor $A$ exists. If the transformations $\boldsymbol{W}$ are chosen in a way such that the attractor $A$ is the original image,
one only needs to store the transformations. For decompression, one can iteratively apply all transformations on any initial starting image, which then converges to the original image to be compressed.\\
A transformation $w_i$ is defined on a source region (called domain block) and a target region (range block) of the image. 
The transformation maps the pixels of the domain block to the pixels of the range block and then applies a brightness and saturation adjustment.\\
A high-level algorithmic approach for compression is then:
\begin{enumerate}
    \item Partition the image into range blocks $\boldsymbol{R}$ and domain blocks $\boldsymbol{D}$.
    \item For each range block $R_i \in \boldsymbol{R}$, find a transformation $w_i$ with domain block $D_i \in \boldsymbol{D}$ 
            such that $w_i(D_i)$ approximates the pixel values of $R_i$ best. In order that $w_i$ is contractive, one must require that $D_i$ is larger than $R_i$.
\end{enumerate}
\mypar{Mathematical view}
A grayscale image can be interpreted as a function
\begin{align*}
    f \colon I^2 &\to I\\
    (x,y) &\mapsto z = f(x,y)
\end{align*}
where $I$ is the interval $[0,1]$. Specifically, an image is a set of three-tuples $(x,y,z) \in I^3$, where $x,y$ are positions and $z$ is the grayscale value.
The image of a range (or domain) block $R_i$ is then defined by $f \cap (R_i \times I)$, where $R_i$ is a continuous region of grayscale values. 
A transformation $w_i$ from a domain block $D_i$ to a range block $R_i$ can then be described as a linear affine transformation
$$
w_i(D_i) = \bigcup_{(x,y,z) \in D_I} \begin{pmatrix} a_i & b_i & 0 \\ c_i & d_i & 0 \\ 0 & 0 & s_i \end{pmatrix} + \begin{pmatrix} x\\y\\z \end{pmatrix} + \begin{pmatrix} e_i\\f_i\\o_i \end{pmatrix}
$$
where $a_i, b_i, c_i, d_i, e_i, f_i$ map \textit{location} and $s_i, o_i$ correspond to contrast and brightness adjustments.
\\
We now can define a metric to compare two images (or two blocks). Usually, the root mean square error (RMS) metric is used:
$$
d_{RMS}(f,g) = \sqrt{\int_{I^2} (f(x,y)-g(x,y))dxdy}
$$
For a range block $R_i$ and domain block $D_i$ we now seek the transformation $w_i$, such that $d_{RMS}(w_i(D_i), f \cap (R_i \times I))$ is minimal, i.e.
$$
    w_i = \argmin_{w} \left( d_{RMS}(w(D_i), f \cap (R_i \times I)) \right)
$$
Calculating $w_i$ is a minimization problem, because we may choose $s_i$ and $o_i$ in order to minimize the error.
Furthermore, because a domain block is defined over a bigger region of the image than a range block, $w_i$ is then contractive. \\
The set of all transformations is a partitioned iterated function system (PIFS). The only difference to an IFS is that the transformations are restricted
to \textit{blocks} (partitions) of the image. However, the underlying theory still holds, as described in \cite{fisher2012}.

\mypar{Finding transformations in practice}
In practice an image consists of pixels, where each pixel has a grayscale value between 0 and 1 and integer coordinates.
The minimization problem described before can be solved using least squares regression, as done in TODO MAKE REFERENCE TO GITHUB PROJECT.
Fisher uses an analytical solution in \cite{fisher2012} to compute the adjustments directly. Let $b_1, ..., b_n$ be the $n$ pixel values of the range block $R_i$
and $a_i,...,a_n$ be the $n$ pixel values of the downsampled domain block. We then seek $s$ (saturation) and $o$ (brightness) such that
$$
R = \sum_{i=1}^n (s \cdot a_i + o - b_i)^2
$$
is minimal. An analytical solution for $s,o,R$ is:
\begin{align*}
    s &= \frac{n \sum_{i=1}^n a_i bi - \sum_{i=1}^n ai \sum_{i=1}^n bi}{n \sum_{i=1}^n ai^2 - \left(\sum{i=1}^n ai \right)^2} \\
    o &= \frac{1}{n} \left( \sum_{i=1}^n bi - s \sum_{}{i=1}^n ai \right) \\
    R &= \frac{1}{n} \left[ \sum_{i=1}^n bi^2 + s \left( s \sum_{i=1}^n ai^2 - 2 \sum_{i=1}^n a_i bi + 2o \sum_{i=1}^n ai \right) \right. \\
    &\text{\hspace{1cm}} \left. + o \left(no - 2 \sum_{i=1}^n b_i \right)  \right]
    \end{align*}
Thus, to compute an optimal transformation $w_i$ and the error for a given range block $R_i$ and given domain block $D_i$, one needs to compute the sums above
as efficient as possible.

\mypar{Image partitioning}
There are many ways to partition an image into range and domain blocks. We require that the range blocks cover the full image and do not overlap, otherwise the
decompressed image would have uncovered regions. The domain blocks represent the search space for self-similarity and may overlap. 
Increasing the domain block pool $\boldsymbol{D}$ may lead to better transformations but can increase the compression performance significantly.
As a simple approach, one can use brute force block mapping, where each range block is compared with each domain block. 
There are other techniques where TODO elaborate
Blocks can be of various sizes and shapes (quadratic, rectangular), the only requirement is that domain blocks are larger than the range blocks to satisfy the
contractivity requirement. The most basic version is to choose range blocks of size $s \times s$ and domain blocks of size $k\cdot s \times k \cdot s$, for some
integer constant $k$. In practice, quadtree partitioning is often used (TODO CITE), which dynamicaly adapts range and domain block sizes by a
predefined error threshold $\epsilon$. We start with some initial (quadratic) range blocks, and domain blocks which
are twice the size. When there is a range block for which all domain blocks exceed the predefined threshold, we partition
the range block into 4 smaller blocks and try to cover them again with domain blocks which are four times the size of these range blocks.
This leads to good compression results in terms of quality and compression size (TODO cite). 



Give a short, self-contained summary of necessary
background information on the algorithm or application that you then later optimize including a cost analysis.

For example, assume you present an
implementation of FFT algorithms. You could organize into DFT
definition, FFTs considered, and cost analysis. The goal of the
background section is to make the paper self-contained for an audience
as large as possible. As in every section
you start with a very brief overview of the section. Here it could be as follows: In this section
we formally define the discrete Fourier transform, introduce the algorithms we use
and perform a cost analysis.

\mypar{Discrete Fourier Transform}
Precisely define the transform so I understand it even if I have never
seen it before.

\mypar{Fast Fourier Transforms}
Explain the algorithm you use.

\mypar{Cost Analysis}
First define you cost measure (what you count) and then compute or determine on other ways the
cost as explained in class. In the end you will likely consolidate it into one number (e.g., adds/mults/comparisons) but be aware of major imbalances as they affect the peak performance..

Also state what is known about the complexity (asymptotic usually)
about your problem (including citations).