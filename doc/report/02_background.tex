\section{Background}\label{sec:background}

To introduce the algorithm, we first present the essential parts of the
underlying mathematical theory, show how it can be used to compress images and
what practical considerations must be made for the implementation as shown by
Fischer \cite{fisher2012}. A cost analysis can be found at the end of this
section. Throughout this chapter, we use the notation from Fisher
\cite{fisher2012}.

\mypar{Iterated function systems (IFS)} Fractal image compression builds on the
theory of iterated function systems. For completeness, we provide a short
introduction to this rich topic.

\begin{definition}[Contractivity of functions]
    A function $f$ on a metric space $X$ with metric $d$ is contractive, when there exists some $0 \leq k < 1$
    such that for all $x,y \in X: d(f(x), f(y)) \leq k \cdot d(x,y)$.
\end{definition}

As an informal example, when $f$ is a contractive function on $\R$, then mapping
two numbers brings them \textit{closer} together, i.e. their distance gets
smaller.

\begin{definition}[Iterated function system]
  An iterated function system (IFS) is a set of $n$ functions
  \begin{align*}
    \{f_i:X \to X | i=1,...,n\}
  \end{align*}
  where each $f_i$ is contractive and $X$ is a metric space with the metric $d$.
\end{definition}

\begin{definition}[Hutchinson operator]
    The Hutchinson operator for an IFS is the function
    \begin{align*}
        F \colon 2^X &\to 2^X\\
        x &\mapsto \bigcup_{i=1}^n f_i(x)
    \end{align*}
\end{definition}

With $F^{\circ n}(x)$, we denote the iterative application of $F$ $n$ times on
its input $x$, e.g. $F^{\circ 2}(x) = F(F(x))$.

\begin{definition}[Attractor of an IFS]
    Let $A \in X$. When $F(A)=A$, then $A$ is called an attractor.
\end{definition}

Hutchinson used the following theorem in his work \cite{hutchinson1981fractals}
to build up the essential theoretical parts of how fractal image compression works.

\begin{theorem}[Contractive mapping fixed-point theorem]\label{theorem-fixpoint}
  For an IFS on a set $X$ it holds that:

  \begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item There always exists a unique attractor $A \in X$
    \item For any nonempty set $S_0 \subset X$ it holds that
      $A = \lim_{n \to \infty} F^{\circ n}(S_0)$
  \end{enumerate}
\end{theorem}

Note that in the theorem above, we require that $X$ is a compact set, which
holds for fractal image compression.

\notepascal{Wenn wir das voraussetzen sollte das nicht auch im Theorem stehen?
  ``...For an IFS on a compact set X...''}

\mypar{From IFS to image compression} The key idea in fractal image compression
is that one computes a set of contractive functions $w_i \in \boldsymbol{W}$ (in
this context called transformations) on the image. From theorem
\ref{theorem-fixpoint}, we then know that some unique attractor $A$ exists. If
the transformations $\boldsymbol{W}$ are chosen in a way such that the attractor
$A$ is the original image, one only needs to store the transformations. For
decompression, one can iteratively apply all transformations on any initial
starting image, which then converges to the original image to be compressed.

A transformation $w_i$ is defined on a contiguous source region (called domain
block) and a contiguous target region (range block) of the image. The
transformation maps the pixels of the domain block to the pixels of the range
block and then applies a brightness and saturation adjustment.

The transformations can then be computed with the following steps.

\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
  \item Partition the image into range blocks $\boldsymbol{R}$ and domain blocks
    $\boldsymbol{D}$.
  \item For each range block $R_i \in \boldsymbol{R}$, find a transformation
    $w_i$ with domain block $D_i \in \boldsymbol{D}$ such that $w_i(D_i)$
    approximates the pixel values of block $R_i$ best.
\end{enumerate}

Finding the best transformation by comparing all domain/range block pairs is
called exhaustive block mapping. More elaborate versions like archetype
classification restrict possible domain block candidates for a given range block
to speed up compression \cite{jacobs1992image}.

\mypar{Transformations} A grayscale image can be interpreted as a function
\begin{align*}
    f \colon I^2 &\to I\\
    (x,y) &\mapsto z = f(x,y)
\end{align*}

where $I$ is the interval $[0,1]$. Specifically, an image is a set of three-tuples $(x,y,z) \in I^3$, where $x,y$ are positions and $z$ is the grayscale value.
The image of a range (or domain) block $R_i$ is then defined by $f \cap (R_i \times I)$.
A transformation $w_i$ from the image of a domain block $D_i$ to the image of a range block $R_i$ can then be described as a linear affine transformation
\begin{align*}
w_i(D_i) = \bigcup_{(x,y,z) \in f \cap (D_i \times I)} \begin{pmatrix} a_i & b_i & 0 \\ c_i & d_i & 0 \\ 0 & 0 & s_i \end{pmatrix} \begin{pmatrix} x\\y\\z \end{pmatrix} + \begin{pmatrix} e_i\\f_i\\o_i \end{pmatrix}
\end{align*}

where $a_i, b_i, c_i, d_i, e_i, f_i$ map \textit{location} (which can involve rotations) and $s_i, o_i$ correspond to contrast and brightness adjustments.
To ensure that $w_i$ is contractive, $D_i$ has to be larger than $R_i$ (in terms of region sizes) and $0 \leq s_i < 1$.

Now we define a metric to compare two images (or two blocks). Usually, the root mean square error (RMS) metric is used:
\begin{align*}
d_{RMS}(f,g) = \sqrt{\int_{I^2} (f(x,y)-g(x,y))dxdy}
\end{align*}
For a range block $R_i$ and domain block $D_i$ we now seek the best
transformation $w_i$:
\begin{align}
  w_i = \argmin_{w} \left( d_{RMS}(w(D_i), f \cap (R_i \times I)) \right)\label{eq:min-problem}
\end{align}

Calculating $w_i$ is a minimization problem, because we may choose $s_i$ and
$o_i$ in order to minimize the error.

The set of all transformations is a partitioned iterated function system (PIFS).
The only difference to an IFS is that the transformations are restricted to
\textit{blocks} (partitions) of the image. However, the underlying theory still
holds \cite{fisher2012}.

\mypar{Computing Transformations}\label{par-practical-implementation}
In practice, an image consists of pixels, where each pixel has a grayscale value between 0 and 1 and integer coordinates.

The minimization problem in equation~\eqref{eq:min-problem} can be solved using least squares regression, as done in \cite{github-python}.
Fisher uses an analytical solution in \cite{fisher2012} to compute the adjustments directly. Let $b_1, ..., b_n$ be the $n$ pixel values of the range block $R_i$
and $a_1,...,a_n$ be the $n$ pixel values of the downsampled and rotated domain block. We then seek $s$ (saturation) and $o$ (brightness) such that
\begin{align*}
  R = \sum_{i=1}^n (s \cdot a_i + o - b_i)^2
\end{align*}
is minimal. An analytical solution for $s,o,R$ is:
\begin{align}
    s &= \frac{n \sum_{i=1}^n a_i b_i - \sum_{i=1}^n a_i \sum_{i=1}^n b_i}{n \sum_{i=1}^n a_i^2 - \left(\sum_{i=1}^n a_i \right)^2} \label{contrast}\\
    o &= \frac{1}{n} \left( \sum_{i=1}^n b_i - s \sum_{i=1}^n a_i \right) \label{brightness}\\
    R &= \frac{1}{n} \left[ \sum_{i=1}^n b_i^2 + s \left( s \sum_{i=1}^n a_i^2 - 2 \sum_{i=1}^n a_i b_i + 2o \sum_{i=1}^n a_i \right) \right. \notag\\
    &\text{\hspace{1cm}} \left. + o \left(no - 2 \sum_{i=1}^n b_i \right)  \right]\label{error}
\end{align}
Thus, to compute an optimal transformation $w_i$ and the error for a given range
block $R_i$ and given domain block $D_i$, one needs to compute the sums above as
efficiently as possible.

\mypar{Image Partitioning} There are many ways to partition an image into range
and domain blocks. We require that the range blocks cover the full image and do
not overlap, otherwise the decompressed image would have uncovered regions. The
domain blocks represent the search space for self-similarity and may overlap.
Increasing the domain block pool $\boldsymbol{D}$ may lead to better
transformations but can increase the compression performance significantly.
Blocks can be of various sizes and shapes (quadratic, rectangular, etc.), but
domain blocks must be larger than range blocks to satisfy the contractivity
requirement. For example, the code in \cite{github-python}, uses quadratic range
blocks of size $s \times s$ and domain blocks of size
$k\cdot s \times k \cdot s$ for some integer constant $k > 1$.

\mypar{Quadtree Partitioning} In practice, quadtree partitioning is considered
a reasonable reference point for more advanced partitioning schemes
\cite{fisher2012}. It dynamically adapts range and domain block sizes by a
predefined error threshold $\epsilon$. We start with some initial (quadratic)
range blocks of size $s \times s$, and domain blocks of size $2s \times 2s$.
When there is a range block for which all domain blocks exceed the threshold
$\epsilon$, we partition the range block into 4 smaller blocks of size
$s/2 \times s/2$ and try to cover them again with domain blocks of size
$s \times s$. This scheme has the effect that (potentially larger) homogeneous
regions of the image are covered with few transformations while image regions
with details are covered with transformations that map smaller blocks. Usually,
one defines also a maximum depth of the quadtree as described in
\cite{fisher2012}.

\mypar{Cost analysis} In this course, we focus on floating point operations and
ignore all integer calculations (e.g. index computation). We count only floating
point multiplications, divisions, additions, and subtractions because no special
mathematical functions are used. The cost metric is defined as
\begin{align}
  C = N_{mult} + N_{div} + N_{add} + N_{sub} \label{eq:cost}
\end{align}

The cost of the algorithm does not only depend on the size but also on the content
of an image. To obtain a rough estimate of the runtime, we provide a
pessimistic upper bound on the cost of the algorithm. We assume that no transformation
will yield an error below the threshold and thus the only termination criteria is the
maximum quadtree depth. In practice, the cost is (much) lower assuming a reasonable
error threshold $\epsilon$.

Let $m$ be the maximum quadtree depth and let $s$ be the width of the square
input image. We assume that we start quadtree with range blocks of size
$\frac{s}{2}\times\frac{s}{2}$ and one domain block of size $s \times s$. For
quadtree depth $i \in \{1, \dots, m\}$, let $R^{(i)}$ be the set of range
blocks, $D^{(i)}$ the set of domain blocks and $n_{i}$ the number of pixels of
one range block. Note that the number of pixels of a domain block is then always
$4\cdot n_{i}$ due to quadtree partitioning and $n_i=\frac{s^2}{|R^{(i)}|} = \frac{s^2}{2^{2i}}$.
For a given domain block and range block in quadtree depth $i$,
let $\alpha_{i}$ denote the cost to compute four transformations $w_i^{0}$,
$w_i^{90}$, $w_i^{180}$, $w_i^{270}$ and their errors. Each transformation
$w_i^{\gamma}$ represents a rotation of the domain block by $\gamma$ degrees.

To calculate $\alpha_{i}$, we need to compute all sums for brightness,
contrast and error (equations \eqref{contrast}, \eqref{brightness} and
\eqref{error}) and apply the formulas. Besides, we need to downscale and
rotate the domain block.

Downscaling one domain block can be done in $4\cdot n_{i}$ flops because we have
to aggregate the average of four pixels of the domain block into one pixel which
requires 4 flops for every square of four pixels.

The sum $\sum_{j=1}^{n_{i}} a_j b_j$ has to be computed for each rotation which
requires $2\cdot n_{i}-1$ flops per rotations. The results of the remaining sums
($\sum_{j=1}^{n_{i}} a_j$, $\sum_{j=1}^{n_{i}} a_j^2$,
$\sum_{j=1}^{n_{i}} b_j$,$\sum_{j=1}^{n_{i}} b_j^2$) are identical for all four
rotations and and require a total of $6\cdot n_{i}-4$ floating point operations.
Applying the formulas for brightness, contrast and the error involves $26$ flops
using the precomputed sums. Therefore,
\begin{align*}
  \alpha_i &= 4\cdot n_{i} + 4 \cdot (2\cdot n_{i}-1) + (6\cdot n_{i}-4) +  26\\
           &= 18\cdot n_{i} + 18 \\
           &= 18 \cdot \frac{s^2}{2^{2i}} + 18
\end{align*}
Thus, we get the following upper bound on the cost:
\begin{align*}
C &\leq \sum_{i=1}^m |R^{(i)}| \cdot |D^{(i)}| \cdot \alpha_i \\
  &= \sum_{i=1}^m 2^{2i} \cdot 2^{2i-2} \cdot (18 \cdot \frac{s^2}{2^{2i}} + 18) \\
  &= \frac{6}{5}\cdot (4^{m}-1)\cdot (4^{m+1} + 5\cdot s^{2} + 4)
\end{align*}
We see that the cost of the algorithm is exponential in the quadtree depth $m$
and polynomial in image width $s$. For the roofline plot and the other benchmarks,
we determined the cost empirically by instrumenting the code.

